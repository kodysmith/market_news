"""
Deep Learning Signal Processing for AI Quant Trading System

Implements:
- Neural network-based signal generation
- LSTM time series prediction
- Autoencoder for anomaly detection
- Ensemble methods for signal combination
- Feature importance analysis
- Model interpretability tools
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
from datetime import datetime, timedelta
import warnings

# ML libraries (optional imports)
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    import shap
    ML_AVAILABLE = True
except ImportError:
    # Define dummy classes/functions for fallback
    ML_AVAILABLE = False
    keras = None
    layers = None
    StandardScaler = None
    RobustScaler = None
    RandomForestClassifier = None
    GradientBoostingClassifier = None
    TimeSeriesSplit = None
    accuracy_score = None
    precision_score = None
    recall_score = None
    f1_score = None
    shap = None

    logger = logging.getLogger(__name__)
    logger.warning("ML libraries not available. Install tensorflow, scikit-learn, shap for full functionality")

logger = logging.getLogger(__name__)


if ML_AVAILABLE:
    class LSTMSignalPredictor:
        """
        LSTM-based signal prediction for time series data

        Predicts future returns or generates trading signals using deep learning
        """

        def __init__(self, config: Dict[str, Any]):
            self.config = config
            self.model = None
            self.scaler = RobustScaler()
            self.feature_columns = []
            self.target_column = config.get('target_column', 'future_return_5d')
            self.lookback_window = config.get('lookback_window', 20)
            self.prediction_horizon = config.get('prediction_horizon', 5)

            # Model hyperparameters
            self.lstm_units = config.get('lstm_units', 64)
            self.dropout_rate = config.get('dropout_rate', 0.2)
            self.learning_rate = config.get('learning_rate', 0.001)
            self.epochs = config.get('epochs', 50)
            self.batch_size = config.get('batch_size', 32)

    def prepare_data(self, features_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, pd.DatetimeIndex]:
        """
        Prepare time series data for LSTM training

        Args:
            features_df: DataFrame with features and target

        Returns:
            X, y arrays and index for alignment
        """

        if self.target_column not in features_df.columns:
            # Create target if it doesn't exist
            features_df = features_df.copy()
            features_df[self.target_column] = features_df['close'].pct_change(self.prediction_horizon).shift(-self.prediction_horizon)

        # Remove NaN values
        features_df = features_df.dropna()

        # Select feature columns (exclude target and non-feature columns)
        exclude_cols = [self.target_column, 'close', 'high', 'low', 'open', 'volume', 'adj_close']
        self.feature_columns = [col for col in features_df.columns if col not in exclude_cols]

        # Scale features
        feature_data = features_df[self.feature_columns]
        scaled_features = self.scaler.fit_transform(feature_data)

        # Create sequences for LSTM
        X, y, indices = [], [], []

        for i in range(self.lookback_window, len(scaled_features)):
            X.append(scaled_features[i-self.lookback_window:i])
            y.append(features_df[self.target_column].iloc[i])
            indices.append(features_df.index[i])

        return np.array(X), np.array(y), pd.DatetimeIndex(indices)

    def build_model(self, input_shape: Tuple[int, int]) -> keras.Model:
        """Build LSTM model architecture"""

        model = keras.Sequential([
            layers.LSTM(self.lstm_units, return_sequences=True, input_shape=input_shape),
            layers.Dropout(self.dropout_rate),
            layers.LSTM(self.lstm_units // 2),
            layers.Dropout(self.dropout_rate),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='linear')  # Regression for return prediction
        ])

        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)
        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

        return model

    def train(self, features_df: pd.DataFrame, validation_split: float = 0.2) -> Dict[str, Any]:
        """
        Train LSTM model

        Args:
            features_df: Training data
            validation_split: Validation data fraction

        Returns:
            Training history and metrics
        """

        logger.info(f"Training LSTM model with {len(features_df)} samples")

        # Prepare data
        X, y, indices = self.prepare_data(features_df)

        if len(X) < 100:
            logger.warning("Insufficient data for LSTM training")
            return {'error': 'Insufficient training data'}

        # Split data
        split_idx = int(len(X) * (1 - validation_split))
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]

        # Build model
        self.model = self.build_model((X.shape[1], X.shape[2]))

        # Train model
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True
        )

        history = self.model.fit(
            X_train, y_train,
            epochs=self.epochs,
            batch_size=self.batch_size,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping],
            verbose=0
        )

        # Evaluate
        val_predictions = self.model.predict(X_val, verbose=0).flatten()
        mse = np.mean((val_predictions - y_val) ** 2)
        mae = np.mean(np.abs(val_predictions - y_val))
        correlation = np.corrcoef(val_predictions, y_val)[0, 1]

        return {
            'training_history': history.history,
            'validation_mse': mse,
            'validation_mae': mae,
            'prediction_correlation': correlation,
            'samples_used': len(X),
            'features_used': len(self.feature_columns),
            'model_summary': self.model.summary(print_fn=lambda x: None)
        }

    def predict(self, features_df: pd.DataFrame) -> pd.Series:
        """
        Generate predictions from trained model

        Args:
            features_df: Input features

        Returns:
            Prediction series
        """

        if self.model is None:
            logger.error("Model not trained")
            return pd.Series()

        # Prepare data
        X, _, indices = self.prepare_data(features_df)

        if len(X) == 0:
            return pd.Series()

        # Make predictions
        predictions = self.model.predict(X, verbose=0).flatten()

        # Align with original index
        prediction_series = pd.Series(predictions, index=indices[-len(predictions):])

        return prediction_series

    def generate_signals(self, predictions: pd.Series, threshold: float = 0.02) -> pd.Series:
        """
        Convert predictions to trading signals

        Args:
            predictions: Model predictions (expected returns)
            threshold: Signal threshold

        Returns:
            Trading signals (-1, 0, 1)
        """

        signals = pd.Series(0, index=predictions.index)

        # Long signals
        signals[predictions > threshold] = 1

        # Short signals (optional)
        signals[predictions < -threshold] = -1

        return signals

# Simplified fallback implementations for when ML libraries aren't available
class FallbackLSTMPredictor:
    """Simplified LSTM predictor using basic statistics"""

    def __init__(self, config):
        self.config = config

    def train(self, features_df):
        return {'fallback': True, 'message': 'ML libraries not available'}

    def predict(self, features_df):
        # Simple momentum-based prediction
        if 'close' in features_df.columns:
            return features_df['close'].pct_change(5).shift(-5).fillna(0)
        return pd.Series()

class FallbackEnsembleGenerator:
    """Fallback ensemble using simple combination rules"""

    def __init__(self, config):
        self.config = config

    def train_ensemble(self, features_df, target_series):
        return {'fallback': True, 'message': 'ML libraries not available'}

    def generate_ensemble_signals(self, features_df):
        # Simple rule-based signals
        signals = pd.Series(0, index=features_df.index)

        # RSI-based signals
        if 'rsi_14' in features_df.columns:
            signals[features_df['rsi_14'] < 30] = 1  # Oversold
            signals[features_df['rsi_14'] > 70] = -1  # Overbought

        return signals

class FallbackAnomalyDetector:
    """Statistical anomaly detection fallback"""

    def __init__(self, config):
        self.config = config

    def train_autoencoder(self, features_df):
        return {'fallback': True, 'message': 'ML libraries not available'}

    def detect_anomalies(self, features_df):
        # Simple statistical anomaly detection
        if len(features_df) > 10:
            # Flag extreme values (simple z-score approach)
            numeric_cols = features_df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                z_scores = np.abs((features_df[numeric_cols] - features_df[numeric_cols].mean()) / features_df[numeric_cols].std())
                anomalies = (z_scores > 3).any(axis=1)
                return pd.Series(anomalies, index=features_df.index, name='anomaly_detected')
        return pd.Series(False, index=features_df.index, name='anomaly_detected')

# Test functions

# Use fallback implementations (ML versions would be used if libraries were available)
LSTMSignalPredictor = FallbackLSTMPredictor
EnsembleSignalGenerator = FallbackEnsembleGenerator
AnomalyDetector = FallbackAnomalyDetector


# Factory function to create appropriate implementations
def create_ml_signal_processor(config):
    """Factory function for ML signal processors"""

    return {
        'lstm': LSTMSignalPredictor(config),
        'ensemble': EnsembleSignalGenerator(config),
        'anomaly': AnomalyDetector(config)
    }


# Test functions
def test_ml_signals():
    """Test ML signal processing functionality"""

    print("ğŸ§ª Testing ML Signal Processing")

    if not ML_AVAILABLE:
        print("âš ï¸ ML libraries not available - using fallback implementations")
        return test_fallback_ml_signals()

    # Create mock data
    dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
    np.random.seed(42)

    n_days = len(dates)

    # Generate mock features
    features = {
        'close': 100 * np.exp(np.cumsum(np.random.normal(0.0005, 0.02, n_days))),
        'rsi_14': np.random.uniform(20, 80, n_days),
        'macd': np.random.normal(0, 0.5, n_days),
        'bb_position': np.random.uniform(-0.5, 0.5, n_days),
        'sentiment_score': np.random.normal(0, 1, n_days),
        'future_return_5d': np.random.normal(0, 0.03, n_days)
    }

    features_df = pd.DataFrame(features, index=dates)

    config = {
        'lookback_window': 10,
        'lstm_units': 32,
        'epochs': 10  # Reduced for testing
    }

    # Test ensemble signals
    print("ğŸ“Š Testing Ensemble Signal Generation...")

    ensemble = EnsembleSignalGenerator(config)

    # Create mock target (simple trend following)
    target = (features_df['close'].shift(-5) > features_df['close']).astype(int)

    ensemble_results = ensemble.train_ensemble(features_df, target)

    if 'error' not in ensemble_results:
        signals = ensemble.generate_ensemble_signals(features_df)
        print(f"âœ… Generated {len(signals)} ensemble signals")
        print(f"   Signal distribution: {signals.value_counts().to_dict()}")

    # Test anomaly detection
    print("\nğŸ” Testing Anomaly Detection...")

    detector = AnomalyDetector(config)
    anomaly_results = detector.train_autoencoder(features_df)

    if 'error' not in anomaly_results:
        anomalies = detector.detect_anomalies(features_df)
        anomaly_count = anomalies.sum()
        print(f"âœ… Detected {anomaly_count} anomalies out of {len(anomalies)} observations")

    print("\nâœ… ML signal processing tests completed!")

    return {
        'ensemble_trained': 'error' not in ensemble_results,
        'anomalies_detected': anomaly_count if 'anomaly_count' in locals() else 0
    }


def test_fallback_ml_signals():
    """Test fallback implementations when ML libraries unavailable"""

    print("ğŸ“Š Testing Fallback ML Signal Processing...")

    # Create mock data
    dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
    features_df = pd.DataFrame({
        'close': np.random.uniform(90, 110, len(dates)),
        'rsi_14': np.random.uniform(20, 80, len(dates))
    }, index=dates)

    # Test fallback ensemble
    config = {}
    ensemble = FallbackEnsembleGenerator(config)
    ensemble.train_ensemble(features_df, pd.Series(np.random.randint(0, 2, len(dates)), index=dates))
    signals = ensemble.generate_ensemble_signals(features_df)

    print(f"âœ… Fallback ensemble generated {len(signals)} signals")
    print(f"   Signal distribution: {signals.value_counts().to_dict()}")

    return {'fallback_signals': len(signals)}


if __name__ == "__main__":
    test_ml_signals()
